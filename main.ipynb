{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe11fe6",
   "metadata": {},
   "source": [
    "# Cofounder Matching Notebook Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Jupyter Notebook provides a workflow for matching potential cofounders based on their backgrounds, skills, interests, and other relevant information. The process leverages multilingual sentence embeddings and similarity scoring to recommend the best individual and group matches from a dataset of cofounder profiles.\n",
    "\n",
    "The workflow includes:\n",
    "- Data loading and cleaning\n",
    "- Text embedding using a transformer model\n",
    "- Pairwise and groupwise similarity scoring\n",
    "- Output of top matches for individuals and groups\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. **Prepare your data**: Ensure your cofounder profiles are in a CSV file with the required columns (e.g., name, email, skills, interests).\n",
    "2. **Run the notebook cells in order**: Each cell builds upon the previous steps, from data cleaning to embedding and matching.\n",
    "3. **Review the outputs**: The notebook produces CSV files and DataFrames with top matches, which you can analyze or export.\n",
    "4. **Customize parameters**: Adjust weights, group sizes, or matching logic as needed for your use case.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Block Explanations\n",
    "\n",
    "### 1. Model Loading\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-small\")\n",
    "```\n",
    "**Purpose:**  \n",
    "Loads a multilingual transformer model for generating sentence embeddings, enabling semantic comparison of text fields in multiple languages.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Data Loading & Cleaning\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "# Load and clean the cofounder profiles CSV\n",
    "# ... (see code for details)\n",
    "```\n",
    "**Purpose:**  \n",
    "Loads the raw CSV data, standardizes column names, combines relevant fields, cleans text (removes HTML, whitespace, etc.), removes duplicates, and saves a cleaned version for further processing.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Embedding Generation\n",
    "\n",
    "```python\n",
    "def embed_text(texts, prefix=\"\"):\n",
    "    # ...\n",
    "df[\"embedding_looking_for\"] = embed_text(df[\"looking_for\"], prefix=\"query:\")\n",
    "# ... (other embeddings)\n",
    "```\n",
    "**Purpose:**  \n",
    "Generates vector embeddings for key text fields (e.g., what someone is looking for, offering, skills, industries) using the loaded transformer model. These embeddings are used for similarity calculations.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Pairwise Matching\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# ... (compute_match_score and matching loop)\n",
    "```\n",
    "**Purpose:**  \n",
    "Calculates similarity scores between all pairs of profiles based on their embeddings and other criteria (e.g., location). Outputs a DataFrame of best matches for each person.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Group Matching\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# ... (find_top_n_per_person)\n",
    "```\n",
    "**Purpose:**  \n",
    "Finds the best groups of cofounders (e.g., teams of 3) by computing average pairwise match scores for all possible groups. Uses parallel processing for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Group Matching Execution\n",
    "\n",
    "```python\n",
    "import os\n",
    "top_groups_df = find_top_n_per_person(df, group_number=3, top_n=5, max_workers=max_workers)\n",
    "```\n",
    "**Purpose:**  \n",
    "Runs the group matching function and saves the top group matches to a CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Display Group Matches\n",
    "\n",
    "```python\n",
    "top_groups_df\n",
    "```\n",
    "**Purpose:**  \n",
    "Displays the resulting DataFrame of top group matches.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Individual Targeted Matching\n",
    "\n",
    "```python\n",
    "def find_top_matches_for_person(df, target_email, group_size=2, top_n=5, **kwargs):\n",
    "    # ...\n",
    "```\n",
    "**Purpose:**  \n",
    "Finds the best matches for a specific individual, either as pairs or in groups, based on their email address.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Run Targeted Matching\n",
    "\n",
    "```python\n",
    "top_matches = find_top_matches_for_person(\n",
    "    df,\n",
    "    target_email=\"colinmatsinhe@gmail.com\",\n",
    "    group_size=2,\n",
    "    top_n=15\n",
    ")\n",
    "```\n",
    "**Purpose:**  \n",
    "Executes the targeted matching function for a specific person and displays the top matches.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Display Targeted Matches\n",
    "\n",
    "```python\n",
    "top_matches\n",
    "```\n",
    "**Purpose:**  \n",
    "Displays the DataFrame of top matches for the selected individual.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model from the sentence-transformers library to use for multilingual sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b145ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "# Load Google Sheet as a CSV\n",
    "df = pd.read_csv(\"cofounder_profiles_original.csv\")\n",
    "# Step 1: Clean and rename columns\n",
    "df.columns = df.columns.str.strip()  # remove leading/trailing whitespace\n",
    "df = df.rename(columns={\n",
    "    \"Vorname\": \"first_name\",\n",
    "    \"Nachname\": \"last_name\",\n",
    "    \"Wohnort\": \"location\",\n",
    "    \"Aktuelle Branche/Industrie\": \"current_industry\",\n",
    "    \"Brancheninteresse\": \"industry_interest\",\n",
    "    \"Gr√ºndungsstatus\": \"startup_status\",\n",
    "    \"Skills/Hintergrund\": \"skills_background\",\n",
    "    \"Ich suche...\": \"looking_for_1\",\n",
    "    \"Ich suche..\": \"looking_for_2\",\n",
    "    \"Ich biete...\": \"offering_1\",\n",
    "    \"Ich biete..\": \"offering_2\",\n",
    "    \"LinkedIn-Profil\": \"linkedin\",\n",
    "    \"E-Mail-Adresse\": \"email\",\n",
    "    \"Telefonnummer\": \"phone\",\n",
    "    \"Zeitstempel\": \"timestamp\"\n",
    "})\n",
    "\n",
    "# Step 2: Combine the 'looking for' and 'offering' columns\n",
    "df[\"looking_for\"] = df[\"looking_for_1\"].fillna(\"\") + \" \" + df[\"looking_for_2\"].fillna(\"\")\n",
    "df[\"offering\"] = df[\"offering_1\"].fillna(\"\") + \" \" + df[\"offering_2\"].fillna(\"\")\n",
    "\n",
    "# Step 3: Drop the old columns (optional)\n",
    "df = df.drop(columns=[\"looking_for_1\", \"looking_for_2\", \"offering_1\", \"offering_2\"])\n",
    "\n",
    "# Step 4: Replace NaNs with empty strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Step 5: Clean the text data\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unescape(text)  # Convert HTML entities (e.g., &amp;) to normal characters\n",
    "    text = re.sub(r'<[^>]*?>', '', text)  # Remove any HTML tags\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)  # Replace newlines with spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Collapse multiple spaces\n",
    "    return text.strip()\n",
    "# Apply the cleaning function to all relevant columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(sanitize_text)\n",
    "\n",
    "# Step 6: Remove duplicates based on 'email' and 'phone' columns\n",
    "df = df.drop_duplicates(subset=[\"email\", \"phone\"], keep=\"first\")\n",
    "\n",
    "# Step 7: Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv(\"cofounder_profiles_cleaned.csv\", index=False)\n",
    "\n",
    "# Optional: preview the cleaned DataFrame\n",
    "print(df.head(1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b800cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the data\n",
    "def embed_text(texts, prefix=\"\"):\n",
    "    return [model.encode(f\"{prefix} {t}\", convert_to_numpy=True) for t in texts]\n",
    "\n",
    "# Embed 'looking_for' as queries\n",
    "df[\"embedding_looking_for\"] = embed_text(df[\"looking_for\"], prefix=\"query:\")\n",
    "\n",
    "# Embed 'offering' as passages\n",
    "df[\"embedding_offering\"] = embed_text(df[\"offering\"], prefix=\"passage:\")\n",
    "\n",
    "# Embed industry fields\n",
    "df[\"embedding_current_industry\"] = embed_text(df[\"current_industry\"], prefix=\"info:\")\n",
    "df[\"embedding_industry_interest\"] = embed_text(df[\"industry_interest\"], prefix=\"info:\")\n",
    "\n",
    "# Embed skills\n",
    "df[\"embedding_skills\"] = embed_text(df[\"skills_background\"], prefix=\"info:\")\n",
    "df.to_csv(\"cofounder_profiles_embeddings.csv\", index=False)\n",
    "df.to_pickle(\"embedded_profiles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load your embedded DataFrame (if not already loaded)\n",
    "df = pd.read_pickle(\"embedded_profiles.pkl\")\n",
    "\n",
    "def compute_match_score(\n",
    "    row_a,\n",
    "    row_b,\n",
    "    weight_main=0.6,\n",
    "    weight_industry=0.15,\n",
    "    weight_skills=0.2,\n",
    "    location_bonus=0.05\n",
    "):\n",
    "    # --- Main match (looking for vs offering)\n",
    "    sim_a = cosine_similarity(\n",
    "        row_a[\"embedding_looking_for\"].reshape(1, -1),\n",
    "        row_b[\"embedding_offering\"].reshape(1, -1)\n",
    "    )[0][0]\n",
    "\n",
    "    sim_b = cosine_similarity(\n",
    "        row_b[\"embedding_looking_for\"].reshape(1, -1),\n",
    "        row_a[\"embedding_offering\"].reshape(1, -1)\n",
    "    )[0][0]\n",
    "\n",
    "    score_main = (sim_a + sim_b) / 2 * weight_main\n",
    "\n",
    "    # --- Industry match\n",
    "    if weight_industry != 0:\n",
    "        industry_sim = cosine_similarity(\n",
    "            row_a[\"embedding_industry_interest\"].reshape(1, -1),\n",
    "            row_b[\"embedding_current_industry\"].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        score_industry = industry_sim * weight_industry\n",
    "    else:\n",
    "        # If weight_industry is 0, we don't want to compute the industry similarity\n",
    "        score_industry = 0\n",
    "\n",
    "    # --- Skills match (symmetric, how similar their skills are)\n",
    "    if weight_skills != 0:\n",
    "        skills_sim = cosine_similarity(\n",
    "            row_a[\"embedding_skills\"].reshape(1, -1),\n",
    "            row_b[\"embedding_skills\"].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        score_skills = skills_sim * weight_skills\n",
    "    else:\n",
    "        # If weight_skills is 0, we don't want to compute the skills similarity\n",
    "        score_skills = 0\n",
    "\n",
    "    # --- Optional location bonus\n",
    "    bonus = 0\n",
    "    if row_a[\"location\"] and row_b[\"location\"]:\n",
    "        if row_a[\"location\"].strip().lower() == row_b[\"location\"].strip().lower():\n",
    "            bonus += location_bonus\n",
    "\n",
    "    # --- Total score\n",
    "    total_score = score_main + score_industry + score_skills + bonus\n",
    "\n",
    "    return total_score\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx_a, row_a in df.iterrows():\n",
    "    for idx_b, row_b in df.iterrows():\n",
    "        if idx_a == idx_b:\n",
    "            continue  # skip self\n",
    "\n",
    "        score = compute_match_score(row_a, row_b)\n",
    "\n",
    "        results.append({\n",
    "            \"person_a\": row_a[\"first_name\"] + \" \" + row_a[\"last_name\"],\n",
    "            \"person_b\": row_b[\"first_name\"] + \" \" + row_b[\"last_name\"],\n",
    "            \"score\": score,\n",
    "            \"email_a\": row_a[\"email\"],\n",
    "            \"email_b\": row_b[\"email\"],\n",
    "            \"location_a\": row_a[\"location\"],\n",
    "            \"location_b\": row_b[\"location\"]\n",
    "        })\n",
    "\n",
    "# Create a DataFrame of match scores\n",
    "match_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort best matches\n",
    "match_df = match_df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# Optional: top N matches per person\n",
    "top_matches = match_df.groupby(\"person_a\").head(5)\n",
    "top_matches.to_csv(\"top_matches.csv\", index=False)\n",
    "top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5677174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import heapq\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def compute_group_score(group_rows, **kwargs):\n",
    "    \"\"\"Compute average pairwise match score for a group of rows\"\"\"\n",
    "    scores = []\n",
    "    for a, b in combinations(group_rows, 2):\n",
    "        score = compute_match_score(a, b, **kwargs)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)  # average of pairwise scores\n",
    "\n",
    "\n",
    "\n",
    "def score_group_helper(group, kwargs):\n",
    "    score = compute_group_score(group, **kwargs)\n",
    "    group_info = {\n",
    "        f\"name_{i+1}\": f\"{r['first_name']} {r['last_name']}\" for i, r in enumerate(group)\n",
    "    }\n",
    "    group_info.update({\n",
    "        f\"email_{i+1}\": r.get(\"email\", \"\") for i, r in enumerate(group)\n",
    "    })\n",
    "    group_info[\"score\"] = score\n",
    "    # Also include indexes or unique id for reference per person\n",
    "    group_info[\"person_ids\"] = [r['email'] for r in group]  # or any unique identifier per person\n",
    "    return (score, group_info)\n",
    "\n",
    "def find_top_n_per_person(df, group_number=2, top_n=5, max_workers=None, **kwargs):\n",
    "    rows = [row._asdict() for row in df.itertuples(index=False)]\n",
    "    all_groups = list(combinations(rows, group_number))\n",
    "\n",
    "    if max_workers is None:\n",
    "        max_workers = os.cpu_count() or 1\n",
    "\n",
    "    # Dictionary: person_id -> min-heap of (score, group_info)\n",
    "    person_matches = defaultdict(list)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(score_group_helper, group, kwargs): group for group in all_groups}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing groups\"):\n",
    "            score, group_info = future.result()\n",
    "            person_ids = group_info[\"person_ids\"]\n",
    "\n",
    "            for pid in person_ids:\n",
    "                heap = person_matches[pid]\n",
    "                if len(heap) < top_n:\n",
    "                    heapq.heappush(heap, (score, group_info))\n",
    "                else:\n",
    "                    if score > heap[0][0]:\n",
    "                        heapq.heapreplace(heap, (score, group_info))\n",
    "\n",
    "    # Prepare results: flatten to one row per person per match\n",
    "    records = []\n",
    "    for pid, matches in person_matches.items():\n",
    "        for score, group_info in sorted(matches, key=lambda x: x[0], reverse=True):\n",
    "            # Include person id, score, and group info\n",
    "            record = {\n",
    "                \"person_id\": pid,\n",
    "                \"score\": score,\n",
    "            }\n",
    "            record.update(group_info)\n",
    "            records.append(record)\n",
    "\n",
    "    df_results = pd.DataFrame(records)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Detected CPU cores: {os.cpu_count()}\")\n",
    "max_workers = os.cpu_count() - 2 if os.cpu_count() > 2 else 1\n",
    "top_groups_df = find_top_n_per_person(df, group_number=3, top_n=5, max_workers=max_workers)\n",
    "\n",
    "print(top_groups_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "top_groups_df.to_csv(\"top_cofounder_groups.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_groups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_top_matches_for_person(df, target_email, group_size=2, top_n=5, **kwargs):\n",
    "    # Convert DataFrame rows to dicts for processing\n",
    "    rows = [row._asdict() for row in df.itertuples(index=False)]\n",
    "\n",
    "    # Find the target person\n",
    "    target = next((row for row in rows if row[\"email\"] == target_email), None)\n",
    "    if not target:\n",
    "        raise ValueError(f\"No person found with email: {target_email}\")\n",
    "\n",
    "    # Prepare pool of other participants\n",
    "    others = [r for r in rows if r[\"email\"] != target_email]\n",
    "\n",
    "    # Generate all possible groups including the target\n",
    "    all_groups = [tuple([target] + list(comb)) for comb in combinations(others, group_size - 1)]\n",
    "\n",
    "    # Score and keep top_n matches using a min-heap\n",
    "    heap = []\n",
    "    for group in tqdm(all_groups, desc=\"Scoring groups\"):\n",
    "        score = compute_group_score(group, **kwargs)\n",
    "\n",
    "        group_info = {\n",
    "            f\"name_{i+1}\": f\"{r['first_name']} {r['last_name']}\" for i, r in enumerate(group)\n",
    "        }\n",
    "        group_info.update({\n",
    "            f\"email_{i+1}\": r.get(\"email\", \"\") for i, r in enumerate(group)\n",
    "        })\n",
    "        group_info[\"score\"] = score\n",
    "\n",
    "        if len(heap) < top_n:\n",
    "            heapq.heappush(heap, (score, group_info))\n",
    "        else:\n",
    "            if score > heap[0][0]:\n",
    "                heapq.heapreplace(heap, (score, group_info))\n",
    "\n",
    "    # Sort results by descending score\n",
    "    top_groups = [x[1] for x in sorted(heap, key=lambda x: x[0], reverse=True)]\n",
    "    df_top_groups = pd.DataFrame(top_groups)\n",
    "\n",
    "    return df_top_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches = find_top_matches_for_person(\n",
    "    df,\n",
    "    target_email=\"colinmatsinhe@gmail.com\",\n",
    "    group_size=2,\n",
    "    top_n=15\n",
    ")\n",
    "\n",
    "print(top_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24732ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_matches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
